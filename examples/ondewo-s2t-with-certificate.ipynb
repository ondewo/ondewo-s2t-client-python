{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Processing /home/jsteinbauer/ondewo/ondewo-s2t-client-python\n",
      "Collecting grpcio==1.42.0\n",
      "  Downloading grpcio-1.42.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio-reflection==1.42.0\n",
      "  Downloading grpcio_reflection-1.42.0-py3-none-any.whl (15 kB)\n",
      "Collecting grpcio-tools==1.42.0\n",
      "  Downloading grpcio_tools-1.42.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mypy-protobuf==3.00\n",
      "  Downloading mypy_protobuf-3.0.0-py3-none-any.whl (15 kB)\n",
      "Collecting ondewo-client-utils>=0.1.0\n",
      "  Downloading ondewo_client_utils-0.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/Miniconda3-py38_4.8.3-Linux-x86_64/lib/python3.8/site-packages (from grpcio==1.42.0->ondewo-s2t-client==3.1.1) (1.15.0)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/Miniconda3-py38_4.8.3-Linux-x86_64/lib/python3.8/site-packages (from grpcio-tools==1.42.0->ondewo-s2t-client==3.1.1) (49.6.0.post20200814)\n",
      "Collecting types-protobuf>=3.17.4\n",
      "  Downloading types_protobuf-3.19.15-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex==2020.7.14\n",
      "  Downloading regex-2020.7.14-cp38-cp38-manylinux2010_x86_64.whl (672 kB)\n",
      "\u001b[K     |████████████████████████████████| 672 kB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses-json==0.5.2\n",
      "  Downloading dataclasses_json-0.5.2-py3-none-any.whl (23 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Downloading marshmallow-3.15.0-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 10.4 MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
      "Collecting stringcase<2.0.0,==1.2.0\n",
      "  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/Miniconda3-py38_4.8.3-Linux-x86_64/lib/python3.8/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json==0.5.2->ondewo-client-utils>=0.1.0->ondewo-s2t-client==3.1.1) (20.4)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Collecting typing-extensions>=3.7.4\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/Miniconda3-py38_4.8.3-Linux-x86_64/lib/python3.8/site-packages (from packaging->marshmallow<4.0.0,>=3.3.0->dataclasses-json==0.5.2->ondewo-client-utils>=0.1.0->ondewo-s2t-client==3.1.1) (2.4.7)\n",
      "Building wheels for collected packages: ondewo-s2t-client, stringcase\n",
      "  Building wheel for ondewo-s2t-client (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ondewo-s2t-client: filename=ondewo_s2t_client-3.1.1-py2.py3-none-any.whl size=20316 sha256=96775f78f2f5fbcdb0101707b68dd303bd04baa1c45c6e117f8e8b6bd5b9eb7a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k3p96bfe/wheels/68/33/5c/6741336a8de21fa9ea1d4ff5e96fc434fb52c5c39721dc69c7\n",
      "  Building wheel for stringcase (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3576 sha256=4f6e4de22878147494d267a1ea17f5f5c50431f961fe4d846b5c885f32fb2b91\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k3p96bfe/wheels/04/0e/31/bf265c64f2a4d24516e9923f1f6293c3bcbcde75e0d80ab47a\n",
      "Successfully built ondewo-s2t-client stringcase\n",
      "Installing collected packages: grpcio, protobuf, grpcio-reflection, grpcio-tools, types-protobuf, mypy-protobuf, regex, marshmallow, marshmallow-enum, mypy-extensions, typing-extensions, typing-inspect, stringcase, dataclasses-json, ondewo-client-utils, ondewo-s2t-client\n",
      "Successfully installed dataclasses-json-0.5.2 grpcio-1.42.0 grpcio-reflection-1.42.0 grpcio-tools-1.42.0 marshmallow-3.15.0 marshmallow-enum-1.5.1 mypy-extensions-0.4.3 mypy-protobuf-3.0.0 ondewo-client-utils-0.1.0 ondewo-s2t-client-3.1.1 protobuf-3.19.4 regex-2020.7.14 stringcase-1.2.0 types-protobuf-3.19.15 typing-extensions-4.1.1 typing-inspect-0.7.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "! cd .. && python -m pip install --user ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ondewo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5267e1415d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mondewo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms2t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ondewo'"
     ]
    }
   ],
   "source": [
    "import ondewo.s2t.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1634234935903,
     "user": {
      "displayName": "Andreas Rath",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjzXdeqp2hxSIMkaDT5PP1Srh8ARQRU8xQsUZeo=s64",
      "userId": "17066140692478356993"
     },
     "user_tz": -120
    },
    "id": "bcff103f"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import wave\n",
    "\n",
    "from ondewo.s2t import speech_to_text_pb2\n",
    "from ondewo.s2t.client.client import Client\n",
    "from ondewo.s2t.client.client_config import ClientConfig\n",
    "from ondewo.s2t.client.services.speech_to_text import Speech2Text\n",
    "from ondewo.s2t.speech_to_text_pb2 import ListS2tPipelinesRequest, Speech2TextConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ondewo_s2t_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-86414ffef0fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mondewo_s2t_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ondewo_s2t_client'"
     ]
    }
   ],
   "source": [
    "import ondewo_s2t_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = speech_to_text_pb2.ListS2tPipelinesRequest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "registered_only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ce069dda2580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistered_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: registered_only"
     ]
    }
   ],
   "source": [
    "lr.registered_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a client object\n",
    "The example below shows how to create a speech to text client object from a client config. \n",
    "When setting *use_secure_channel=True*, a grpc certificate *grpc_cert* is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1634234935904,
     "user": {
      "displayName": "Andreas Rath",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjzXdeqp2hxSIMkaDT5PP1Srh8ARQRU8xQsUZeo=s64",
      "userId": "17066140692478356993"
     },
     "user_tz": -120
    },
    "id": "585c9e11",
    "outputId": "a032016c-49e1-46c3-bdb9-0187ce8fe72c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using insecure grpc channel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClientConfig(host='localhost', port='50655', grpc_cert=None)\n"
     ]
    }
   ],
   "source": [
    "AUDIO_FILE: str = \"examples/audiofiles/sample_1.wav\"\n",
    "# credentials = grpc.ssl_channel_credentials(root_certificates=cert)\n",
    "\n",
    "MAX_MESSAGE_LENGTH: int = 60000000\n",
    "GRPC_HOST: str = \"localhost\"\n",
    "GRPC_PORT: str = \"50655\"\n",
    "CHANNEL: str = f\"{GRPC_HOST}:{GRPC_PORT}\"\n",
    "grpc_cert: str = None\n",
    "\n",
    "\n",
    "options = [\n",
    "    ('grpc.max_send_message_length', MAX_MESSAGE_LENGTH),\n",
    "    ('grpc.max_receive_message_length', MAX_MESSAGE_LENGTH),\n",
    "]\n",
    "\n",
    "# channel = grpc.secure_channel(CHANNEL, credentials, options=options)\n",
    "\n",
    "config: ClientConfig = ClientConfig(\n",
    "  host=GRPC_HOST,\n",
    "  port=GRPC_PORT, \n",
    "  grpc_cert=grpc_cert)\n",
    "    \n",
    "print(config)\n",
    "    \n",
    "client: Client = Client(config=config, use_secure_channel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all existing speech to text pipelines\n",
    "All relevant configurations of the speech to text server are defined in a speech to text pipeline. \n",
    "A running server can store several such configs at the same time, such that the client can chose which one to \n",
    "pick when he sends a request to transcribe an audio file or stream.\n",
    "\n",
    "The example below shows how to list all available pipelines by calling the **s2t_service.list_s2t_pipelines()** function, which takes a **ListS2tPipelinesRequest()** as an argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1634234936728,
     "user": {
      "displayName": "Andreas Rath",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjzXdeqp2hxSIMkaDT5PP1Srh8ARQRU8xQsUZeo=s64",
      "userId": "17066140692478356993"
     },
     "user_tz": -120
    },
    "id": "PT_wyVxIU8mk",
    "outputId": "72821027-e8c1-4732-d39a-383c78c20ec1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: \"ato001\"\n",
      "description {\n",
      "  language: \"de\"\n",
      "  pipeline_owner: \"ondewo\"\n",
      "  domain: \"itsupport\"\n",
      "}\n",
      "active: true\n",
      "inference {\n",
      "  ctc_acoustic_models {\n",
      "    type: \"wav2vec\"\n",
      "    quartznet {\n",
      "      config_path: \"acoustic_models/model_configs/de/quartznet15x5_de.yaml\"\n",
      "      load_type: \"pt_files\"\n",
      "      pt_files {\n",
      "        path: \"acoustic_models/de/quartznet15x5_de_2/\"\n",
      "        step: \"2500\"\n",
      "      }\n",
      "      ckpt_file {\n",
      "        path: \"acoustic_models/de/ckpt_test/test.ckpt\"\n",
      "      }\n",
      "      use_gpu: true\n",
      "    }\n",
      "    quartznet_triton {\n",
      "      config_path: \"acoustic_models/model_configs/de/quartznet15x5_de.yaml\"\n",
      "      triton_url: \"localhost:8001\"\n",
      "      triton_model: \"quartznet\"\n",
      "    }\n",
      "    wav2vec {\n",
      "      model_path: \"acoustic_models/de/wav2vec2_large_1360h_telephonized/\"\n",
      "      use_gpu: true\n",
      "    }\n",
      "  }\n",
      "  language_models {\n",
      "    path: \"language_models/\"\n",
      "    beam_size: 64\n",
      "    default_lm: \"german_lm_big.binary\"\n",
      "    beam_search_scorer_alpha: 2.0\n",
      "    beam_search_scorer_beta: 1.0\n",
      "  }\n",
      "}\n",
      "streaming_server {\n",
      "  output_style: \"simple\"\n",
      "  streaming_speech_recognition {\n",
      "    ctc_decoding_method: \"BEAM_SEARCH_WITH_LM\"\n",
      "    sampling_rate: 16000\n",
      "    min_audio_chunk_size: 8000\n",
      "    start_of_utterance_threshold: 0.25\n",
      "    end_of_utterance_threshold: 1.5\n",
      "    next_chunk_timeout: 60.0\n",
      "  }\n",
      "}\n",
      "voice_activity_detection {\n",
      "  active: \"pyannote\"\n",
      "  sampling_rate: 16000\n",
      "  pyannote {\n",
      "    model_path: \"vad_models/pyannote/model/train/other/validate\"\n",
      "    min_audio_size: 8000\n",
      "    offset: 0.8500000238418579\n",
      "    onset: 0.8500000238418579\n",
      "    log_scale: true\n",
      "    min_duration_off: 0.009999999776482582\n",
      "    min_duration_on: 0.009999999776482582\n",
      "  }\n",
      "  matchbox {\n",
      "    model_config: \"vad_models/matchbox/matchbox_config.yaml\"\n",
      "    encoder_path: \"vad_models/matchbox/matchbox_encoder.pt\"\n",
      "    decoder_path: \"vad_models/matchbox/matchbox_decoder.pt\"\n",
      "  }\n",
      "}\n",
      "post_processing {\n",
      "  pipeline: \"normalization\"\n",
      "  post_processors {\n",
      "    sym_spell {\n",
      "      dict_path: \"server_assets/spelling_dicts/de_full.txt\"\n",
      "      max_dictionary_edit_distance: 2\n",
      "      prefix_length: 7\n",
      "    }\n",
      "    normalization {\n",
      "      language: \"german\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "id: \"quartznet_de\"\n",
      "description {\n",
      "  language: \"de\"\n",
      "  pipeline_owner: \"ONDEWO\"\n",
      "  domain: \"general\"\n",
      "}\n",
      "active: true\n",
      "inference {\n",
      "  ctc_acoustic_models {\n",
      "    type: \"quartznet\"\n",
      "    quartznet {\n",
      "      config_path: \"acoustic_models/model_configs/de/quartznet15x5_de.yaml\"\n",
      "      load_type: \"pt_files\"\n",
      "      pt_files {\n",
      "        path: \"acoustic_models/de/quartznet15x5_de_2/\"\n",
      "        step: \"5000\"\n",
      "      }\n",
      "      ckpt_file {\n",
      "        path: \"acoustic_models/de/ckpt_test/test.ckpt\"\n",
      "      }\n",
      "      use_gpu: true\n",
      "    }\n",
      "    quartznet_triton {\n",
      "      config_path: \"acoustic_models/model_configs/de/quartznet15x5_de.yaml\"\n",
      "      triton_url: \"localhost:8001\"\n",
      "      triton_model: \"quartznet\"\n",
      "    }\n",
      "    wav2vec {\n",
      "      model_path: \"acoustic_models/en/wav2vec/wav2vec2-base-960h\"\n",
      "      use_gpu: true\n",
      "    }\n",
      "  }\n",
      "  language_models {\n",
      "    path: \"language_models/ngram_lm_models/test/de/\"\n",
      "    beam_size: 64\n",
      "    default_lm: \"de_small\"\n",
      "    beam_search_scorer_alpha: 2.0\n",
      "    beam_search_scorer_beta: 1.0\n",
      "  }\n",
      "}\n",
      "streaming_server {\n",
      "  output_style: \"simple\"\n",
      "  streaming_speech_recognition {\n",
      "    transcribe_not_final: true\n",
      "    ctc_decoding_method: \"BEAM_SEARCH_WITH_LM\"\n",
      "    sampling_rate: 16000\n",
      "    min_audio_chunk_size: 8000\n",
      "    start_of_utterance_threshold: 0.25\n",
      "    end_of_utterance_threshold: 0.75\n",
      "    next_chunk_timeout: 1.0\n",
      "  }\n",
      "}\n",
      "voice_activity_detection {\n",
      "  active: \"pyannote\"\n",
      "  sampling_rate: 16000\n",
      "  pyannote {\n",
      "    model_path: \"vad_models/pyannote/model/train/other/validate\"\n",
      "    min_audio_size: 8000\n",
      "    offset: 0.8500000238418579\n",
      "    onset: 0.8500000238418579\n",
      "    log_scale: true\n",
      "    min_duration_off: 0.009999999776482582\n",
      "    min_duration_on: 0.009999999776482582\n",
      "  }\n",
      "  matchbox {\n",
      "    model_config: \"vad_models/matchbox/matchbox_config.yaml\"\n",
      "    encoder_path: \"vad_models/matchbox/matchbox_encoder.pt\"\n",
      "    decoder_path: \"vad_models/matchbox/matchbox_decoder.pt\"\n",
      "  }\n",
      "}\n",
      "post_processing {\n",
      "  pipeline: \"normalization\"\n",
      "  post_processors {\n",
      "    sym_spell {\n",
      "      dict_path: \"server_assets/spelling_dicts/de_full.txt\"\n",
      "      max_dictionary_edit_distance: 2\n",
      "      prefix_length: 2\n",
      "    }\n",
      "    normalization {\n",
      "      language: \"german\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "id: \"quarznet_en\"\n",
      "description {\n",
      "  language: \"en\"\n",
      "  pipeline_owner: \"ONDEWO\"\n",
      "  domain: \"general\"\n",
      "}\n",
      "active: true\n",
      "inference {\n",
      "  ctc_acoustic_models {\n",
      "    type: \"quartznet\"\n",
      "    quartznet {\n",
      "      config_path: \"acoustic_models/model_configs/en/quartznet15x5.yaml\"\n",
      "      load_type: \"pt_files\"\n",
      "      pt_files {\n",
      "        path: \"acoustic_models/en/quartznet15x5/\"\n",
      "        step: \"247400\"\n",
      "      }\n",
      "      ckpt_file {\n",
      "        path: \"acoustic_models/de/ckpt_test/test.ckpt\"\n",
      "      }\n",
      "      use_gpu: true\n",
      "    }\n",
      "    quartznet_triton {\n",
      "      config_path: \"acoustic_models/model_configs/en/quartznet15x5.yaml\"\n",
      "      triton_url: \"localhost:8001\"\n",
      "      triton_model: \"quartznet\"\n",
      "    }\n",
      "    wav2vec {\n",
      "      model_path: \"acoustic_models/en/wav2vec/wav2vec2-base-960h\"\n",
      "      use_gpu: true\n",
      "    }\n",
      "  }\n",
      "  language_models {\n",
      "    path: \"language_models/ngram_lm_models/test/en/\"\n",
      "    beam_size: 64\n",
      "    default_lm: \"en_small\"\n",
      "    beam_search_scorer_alpha: 2.0\n",
      "    beam_search_scorer_beta: 1.0\n",
      "  }\n",
      "}\n",
      "streaming_server {\n",
      "  output_style: \"simple\"\n",
      "  streaming_speech_recognition {\n",
      "    transcribe_not_final: true\n",
      "    ctc_decoding_method: \"GREEDY\"\n",
      "    sampling_rate: 16000\n",
      "    min_audio_chunk_size: 8000\n",
      "    start_of_utterance_threshold: 0.25\n",
      "    end_of_utterance_threshold: 0.75\n",
      "    next_chunk_timeout: 1.0\n",
      "  }\n",
      "}\n",
      "voice_activity_detection {\n",
      "  active: \"pyannote\"\n",
      "  sampling_rate: 16000\n",
      "  pyannote {\n",
      "    model_path: \"vad_models/pyannote/model/train/other/validate\"\n",
      "    min_audio_size: 8000\n",
      "    offset: 0.8500000238418579\n",
      "    onset: 0.8500000238418579\n",
      "    log_scale: true\n",
      "    min_duration_off: 0.009999999776482582\n",
      "    min_duration_on: 0.009999999776482582\n",
      "  }\n",
      "  matchbox {\n",
      "    model_config: \"vad_models/matchbox/matchbox_config.yaml\"\n",
      "    encoder_path: \"vad_models/matchbox/matchbox_encoder.pt\"\n",
      "    decoder_path: \"vad_models/matchbox/matchbox_decoder.pt\"\n",
      "  }\n",
      "}\n",
      "post_processing {\n",
      "  pipeline: \"normalization\"\n",
      "  post_processors {\n",
      "    sym_spell {\n",
      "      dict_path: \"server_assets/spelling_dicts/en-80k.txt\"\n",
      "      max_dictionary_edit_distance: 2\n",
      "      prefix_length: 2\n",
      "    }\n",
      "    normalization {\n",
      "      language: \"english\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s2t_service: Speech2Text = client.services.speech_to_text\n",
    "pipelines = s2t_service.list_s2t_pipelines(request=ListS2tPipelinesRequest()).pipeline_configs\n",
    "for pipeline in pipelines:\n",
    "    print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GetS2TPipeline, DeleteS2TPipeline, CreateS2TPipeline, UpdateS2TPipeline\n",
    "In the following, we demonstrate how to do CRUD (Create, Retrieve, Update, Delete) pipelines.\n",
    "\n",
    "The example below shows how to list all available pipelines by calling the **s2t_service.get_s2t_pipeline()** function, which takes a **S2tPipelineId** as an argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GetS2TPipeline\n",
    "To get a specific s2t pipeline, we can use the GetS2TPipeline endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Protocol message ListS2tPipelinesRequest has no \"registered_only\" field.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7e629f1f586c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# pipeline = s2t_service.get_s2t_pipeline(request=S2tPipelineId(id=pipeline_id))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpipelines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2t_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_s2t_pipelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mListS2tPipelinesRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregistered_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_configs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of pipelines: {len(pipelines)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Protocol message ListS2tPipelinesRequest has no \"registered_only\" field."
     ]
    }
   ],
   "source": [
    "from ondewo.s2t.speech_to_text_pb2 import S2tPipelineId\n",
    "\n",
    "# ### GetS2TPipeline\n",
    "# To get a specific s2t pipeline, we can use the GetS2TPipeline endpoint.\n",
    "\n",
    "# pipeline_id = 'wav2vec_en'\n",
    "# pipeline = s2t_service.get_s2t_pipeline(request=S2tPipelineId(id=pipeline_id))\n",
    "\n",
    "pipelines = s2t_service.list_s2t_pipelines(request=ListS2tPipelinesRequest(registered_only=True)).pipeline_configs\n",
    "print(f\"Number of pipelines: {len(pipelines)}\")\n",
    "\n",
    "# print(pipeline)\n",
    "\n",
    "\n",
    "# ### DeleteS2TPipeline\n",
    "# To delete specific s2t pipeline, we can use the GetS2TPipeline endpoint.\n",
    "\n",
    "# pipeline_id = 'wav2vec_en'\n",
    "# pipeline = s2t_service.delete_s2t_pipeline(request=S2tPipelineId(id=pipeline_id))\n",
    "\n",
    "# pipelines = s2t_service.list_s2t_pipelines(request=ListS2tPipelinesRequest()).pipeline_configs\n",
    "# print(f\"Number of pipelines after deletion: {len(pipelines)}\")\n",
    "\n",
    "\n",
    "# ### CreateS2TPipeline\n",
    "# # To delete specific s2t pipeline, we can use the GetS2TPipeline endpoint.\n",
    "\n",
    "# pipeline = s2t_service.create_s2t_pipeline(request=pipeline)\n",
    "\n",
    "# pipelines = s2t_service.list_s2t_pipelines(request=ListS2tPipelinesRequest()).pipeline_configs\n",
    "# print(f\"Number of pipelines after deletion: {len(pipelines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ListS2tPipelinesRequest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.registered_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634234936731,
     "user": {
      "displayName": "Andreas Rath",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjzXdeqp2hxSIMkaDT5PP1Srh8ARQRU8xQsUZeo=s64",
      "userId": "17066140692478356993"
     },
     "user_tz": -120
    },
    "id": "IEMZ_gzlW4nK"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "def find_pipeline_for_language(pipelines: List[Speech2TextConfig], language: str) -> Optional[Speech2TextConfig]:\n",
    "    \"\"\" \n",
    "    Returns the first speech to text pipeline for the requested language. \n",
    "    If no pipline is found, return None.\n",
    "    \"\"\"\n",
    "    for pipeline in pipelines:\n",
    "        if pipeline.description.language == language:\n",
    "            return pipeline\n",
    "\n",
    "english_pipeline = find_pipeline_for_language(pipelines=pipelines, language='en')\n",
    "german_pipeline = find_pipeline_for_language(pipelines=pipelines, language='de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in example audio file\n",
    "This audio file will be used in the following transcription examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave \n",
    "\n",
    "audio_file_path = \"audiofiles/sample_1.wav\"\n",
    "\n",
    "with wave.open(audio_file_path) as w:\n",
    "    audio: bytes = w.readframes(w.getnframes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a transcribe file request to the server\n",
    "In general, are two different endpoints for audio transcriptions:\n",
    "1. **Transcribe an audio file** \n",
    "2. **Transcribe an audio stream**\n",
    "\n",
    "### Transcribe an audio file\n",
    "In this example, we create a **TranscribeFileRequest**, including the audio file (as bytes) and a **TranscribeRequestConfig** message, including the speech to text pipeline id, as well as optional additional parameters.\n",
    "The request message is then used as an argument to the **s2t_service.transcribe_file()**, which calls the corresponding endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1055,
     "status": "ok",
     "timestamp": 1634234937777,
     "user": {
      "displayName": "Andreas Rath",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjzXdeqp2hxSIMkaDT5PP1Srh8ARQRU8xQsUZeo=s64",
      "userId": "17066140692478356993"
     },
     "user_tz": -120
    },
    "id": "34fe77f3",
    "outputId": "7221e03d-d89c-457d-a1f8-530c502302f8"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "pipeline: Speech2TextConfig = pipelines[1]\n",
    "\n",
    "request = speech_to_text_pb2.TranscribeFileRequest(\n",
    "    audio_file=audio,\n",
    "    config=speech_to_text_pb2.TranscribeRequestConfig(\n",
    "        s2t_pipeline_id=pipeline.id,\n",
    "        ctc_decoding=speech_to_text_pb2.CTCDecoding.BEAM_SEARCH_WITH_LM,\n",
    "    )\n",
    ")\n",
    "# Send transcription request and get response\n",
    "transcribe_response = s2t_service.transcribe_file(request=request)\n",
    "\n",
    "for transcription_message in transcribe_response.transcriptions:\n",
    "    print(f\"File transcript: {transcription_message.transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe an audio stream\n",
    "In this example, we transcribe an audio stream by streaming a **TranscribeStreamRequest**, including audio chunks (as bytes) and a **TranscribeRequestConfig** message, including the speech to text pipeline id, as well as optional additional parameters.\n",
    "The request message generator is then used as an argument to the **s2t_service.transcribe_stream()**, which calls the corresponding endpoint.\n",
    "\n",
    "**Important**: After the TranscribeRequestConfig has been set once, it does not have to be sent with each new streameing request (this can help to save bandwidth). The old TranscribeRequestConfig remains until a new one is sent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1.: Transcribe full utterances only (default)\n",
    "In this mode, audio chunks are concatenated until a full utterance is accumulated (an utterance is considered \"finished\" if no voice is detected in the audio signal for `end_of_utterance_threshold` seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from streaming_example import get_streaming_audio, create_streaming_request\n",
    "\n",
    "# Get audio stream (iterator of audio chunks)\n",
    "audio_stream: Iterator[bytes] = get_streaming_audio(\"audiofiles/sample_1.wav\")\n",
    "\n",
    "# Create streaming request\n",
    "streaming_request: Iterator[speech_to_text_pb2.TranscribeStreamRequest] = create_streaming_request(\n",
    "    audio_stream=audio_stream, \n",
    "    pipeline_id=pipeline.id,\n",
    "    transcribe_not_final=False,\n",
    ")\n",
    "\n",
    "# Transcribe the stream and get back responses\n",
    "response_gen: Iterator[speech_to_text_pb2.TranscribeStreamResponse] = s2t_service.transcribe_stream(\n",
    "    request_iterator=streaming_request\n",
    ")\n",
    "\n",
    "# Print transcribed utterances\n",
    "for i, response_chunk in enumerate(response_gen):\n",
    "    for transcribe_message in response_chunk.transcriptions:\n",
    "        print(f\"{i}. response_chunk: {transcribe_message.transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1.: Transcribe not final\n",
    "In this mode, audio chunks are transcribed as soon as a minimal length of voice signal is accumulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "from streaming_example import get_streaming_audio, create_streaming_request\n",
    "\n",
    "# Get audio stream (iterator of audio chunks)\n",
    "audio_stream: Iterator[bytes] = get_streaming_audio(\"audiofiles/sample_1.wav\")\n",
    "\n",
    "# Create streaming request\n",
    "streaming_request: Iterator[speech_to_text_pb2.TranscribeStreamRequest] = create_streaming_request(\n",
    "    audio_stream=audio_stream, \n",
    "    pipeline_id=pipeline.id, \n",
    "    transcribe_not_final=True,\n",
    ")\n",
    "\n",
    "# Transcribe the stream and get back responses\n",
    "response_gen: Iterator[speech_to_text_pb2.TranscribeStreamResponse] = s2t_service.transcribe_stream(\n",
    "    request_iterator=streaming_request\n",
    ")\n",
    "\n",
    "# Print transcribed utterances\n",
    "for i, response_chunk in enumerate(response_gen):\n",
    "    for transcribe_message in response_chunk.transcriptions:\n",
    "        print(f\"{i}. response_chunk: {transcribe_message.transcription}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ondewo-s2t-with-certificate.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "de2ed104cf3bee5ac5c3780eed171797f09568a0a669428569335bd376aa9e40"
  },
  "kernelspec": {
   "display_name": "ondewo_s2t_client",
   "language": "python",
   "name": "ondewo_s2t_client"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
